\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
%\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09
%\usepackage{authblk}

\title{10-601B Group Project Final Report}

\author{
Liruoyang, Yu\\
MISM\\
\texttt{magicyuli@cmu.edu}
\and
Yilong, Chang\\
MISM\\
\texttt{yilongc@andrew.cmu.edu}
}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}
\maketitle
\begin{abstract}
This report summarizes the halfway results of the 10-601B image classification group project. This report will introduce the classifiers we implemented, and then focus on the implementation methods and the experiment results.
\end{abstract}

\section{Introduction}
For the final submission, we trained and tested three classifiers, SVM,  Neural Network and k-NN. The dataset we used for training is a subset of 5000 images from CIFAR-10. We used VLFeat to extract HoG from images and then feed the extracted features to our classifiers.

We achieved 63.0\%, 62.7\% and 61.4\% accuracy on Autolab test using these three classifiers respectively.

\subsection{Motivation}

There are three reasons why we chose SVM. First is that image data is usually not linearly separable in the original feature space, and kernel SVM can deal with this kind of dataset pretty well. Second, the training of SVM is a convex optimization problem, which made it efficient to train. Third, SVM has relatively higher resistance to over-fitting because of the maximization of the margin. And it's easy to adjust C to avoid over-fitting.

The other two classification methods are both non-parametric and therefore, are easy to use and understand. Also, Neural Networks work well on recognition problems.

\subsection{Background and Related Work}
\subsubsection{Background}
"Image classification refers to the task of extracting information classes from a multiband raster image". And CIFAR-10 is an established dataset consisting of 60000 32$\times$32 color images in 10 classes used for the purpose of studying Image classification. 

\subsubsection{Related Work}

Kernel SVM has proven to be effective in the image classification field\cite{chapelle1999support} and was the go-to method before Convolutional Neural Network (CNN) exhibited its great power.

In recent years, CNN has been very successful in image classification. Some implementations of CNN have achieved state-of-the-art performance on the CIFAR-10 images, which are our target dataset. These works include a most recent model using fraction max-polling\cite{graham2014fractional}, reduced the error rate to 3.47\%. The Deeply Supervised Networks\cite{lee2014deeply}, by introducing a "companion" objective functions at each individual hidden layer they achieved an accuracy of 91.78\%. The Network in Network\cite{minlin2014network}, in which the authors built micro neural networks to abstract the data within the receptive field and obtained a 91.2\% accuracy.

\section{Method}

\subsection{Feature Engineering}

We used VLFeat to extract the Histogram of Oriented Gradient (HoG) of images, and use them as the features. HoG summarizes the magnitude of gradients along different directions at different positions of a image, which can reflect color and brightness changes.

\subsection{Data Preprocessing}
In order to prevent over-fitting and improve the training efficiency, we applied PCA to the extracted HoG data. Before PCA, there were 2304 feature dimensions. After PCA, we reserved the first 500 components for the Neural Network, which can explain 95\% of the variance, first 200 for SVM, which can explain more than 80\%, and first 143 for k-NN, which can explan approximate 74\%.

\subsection{SVM}

\subsubsection{Training}

Since there are 10 classes of images, we adopted the 1 vs all strategy, i.e. training 10 classifiers based on the 10 labels we have.

We trained each classifier by maximizing the following dual problem of the objective function of SVM
\begin{equation}
W(\alpha)=\sum_{i=1}^{N}\alpha_{i}-\frac{1}{2}\sum_{i,j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}K(\mathbf{x_{i}},\mathbf{x_{j}})
\end{equation}
\begin{equation}
0<\alpha_{i}<C
\end{equation}
,where $\mathbf{x_{i}}$ is the feature vector of a training sample after HoG and PCA, $y_{i}=1$ or $-1$ indicating whether $\mathbf{x_{i}}$ belongs to that class, $K(\mathbf{x_{i}},\mathbf{x_{j}})$ is the kernel trick. And we used the Gaussian Kernel as the kernel function
\begin{equation}
K(\mathbf{x_{1}},\mathbf{x_{2}})=e^{-\frac{||\mathbf{x_{1}}-\mathbf{x_{2}}||^{2}_{2}}{2\sigma^{2}}}
\end{equation}

We used quadprog() in Matlab to find the $\mathbf{\alpha}$ that maximizes function (1), then compute the bias as
\begin{equation}
b=\frac{\sum_{i=1}^{N}\alpha_{i}y_{i}}{\#~of~\alpha_{i}\neq{0}}
\end{equation}

\subsubsection{Predicting}

We predicted the class $c$ for new samples based on
\begin{equation}
c=\arg\max_{c}\sum_{i=1}^{N}\alpha_{i,c}y_{i,c}K(\mathbf{x_{i}},\mathbf{x_{new}})+b_{c}
\end{equation}

\subsection{Neural Network}
In this phase, we first implemented batch mode. Then we upgraded our basic model to a two-hidden-layered Network and compared the performance with the basic one. The boost of adding a layer was not remarkable, but the result proved incremental mode gradient descent was better. After we tested several layer numbers, we decided to use a Network with three hidden layers. Since we used 500 features, the number of input nodes was 500. And the number of nodes for the hidden layers were 150, 150, 50 respectively.

\subsubsection{Training}
During the training process, we used softmax as activation function:
\begin{equation}
p(c_k=1|x) =  \frac{e^{o_k}}{\sum_{j=1}^{C}e^{o_j}}
\end{equation}
where $o_{j}$ is the output for class j, and $o_{j}$ is in range of (0, 1) adding up to 1. C is the number of classes. 

The lost function we used:
\begin{equation}
L(x, y; \theta) = -\sum_jy_j\log p(c_j|x)
\end{equation}

The learning process would be minimizing the loss over the whole training set:
\begin{equation}
\theta^* = \arg\min_\theta\sum_{n=1}^NL(x^n, y^n; \theta)
\end{equation}
Where N is total number of samples in training set. And we would stop iteration once the change rate of  dropped to a threshold.

\subsubsection{Predicting}
We predicted the class of test data with:
\begin{equation}
y=\arg\max_{c}o_{c}
\end{equation}
where $o_{c}$ is the output for class c.

\subsection{k-NN}
\subsubsection{Training}

\subsubsection{Predicting}

\subsection{Evaluation}

We evaluated our classifier by accuracy on the validation set and the test set on Autolab. And when training Neutral Network, we evaluated the model we trained by the RMSE on the training set.

We also leveraged validation to select the optimal hyperparameters as described in section \ref{experiments}, Experiments and Results.
\section{Experiments and results}
\label{experiments}

\subsection{SVM}

\subsubsection{Linear SVM vs Kernel SVM}
We experimented with both Linear SVM and Kernel SVM, and it turned out that Linear SVM achieved best accuracy of 53\% on the validation set, whereas Kernel SVM achieved 61.8\%.

\subsubsection{Hyperparameter Selection}

There are two hyperparameters for Gaussian Kernel SVM, namely $C$ and $\sigma$. We selected these hyperparameters by validation. We separated the 5,000 samples in the training set into one training set of 4,000 samples and one validation set of 1,000 samples.

We selected $\sigma$ and $C$ based on the accuracy on the validation set, as shown in Figure \ref{figure1} for $\sigma$ and Figure \ref{figure2} for $C$. 

The final $\sigma$ and $C$ we selected were 1.9 and 100, which achieved 61.8\% accuracy on the validation set.

\subsubsection{Autolab submission}
Our Kernel SVM classifier achieved 60\% accuracy on Autolab test set.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=90mm]{sigma.jpg}
	\caption{Validation set accuracy under different sigma\label{figure1}}
\end{figure}

\begin{figure}[ht!]
	\centering
	\includegraphics[width=90mm]{C.jpg}
	\caption{Validation set accuracy under different C\label{figure2}}
\end{figure}

\subsection{Neural Network}
Besides the models we introduced in the mid-way report, we tried other techniques to improve the performance of Neural Network. 

In each training process we first initialized the weights vectors $\mathbf{w_i}$, which was the weight vector between input layer and the first hidden layer and also between hidden layers in CNN, and $\mathbf{u}$, the weight vector between the last hidden layer and output layer with random number $\sim$ $N$(0, $N$), where $N$ is the size of training set. 

The next few subsections deal with both the models in the last phase and the new optimization techniques we tried.

\subsubsection{Basic Neural Network}
In the first model, we set a fixed learning rate at:
\begin{equation}
\eta = 2
\end{equation}
We used the 4000 samples to train. Once the change rate of RMSE was less than 0.0005, we would stop the iteration. 

\subsubsection{Regularized Neural Network}
In the second model, we added two bias vectors $\mathbf{b1}$ and  $\mathbf{b2}$ with initial values from [-0.5, 0.5], and updated these two vectors together with weight vectors. 

This time, we set the learning rate at:
\begin{equation}
\eta = \frac{2}{\sqrt{m}},
\end{equation}
where m is the current number of epoch. 

To avoid over-fitting, we regularized using Simple Weight Decay:
\begin{equation}
\mathbf{w}\rightarrow\mathbf{w_0}-\frac{\eta\lambda}{n}
\end{equation}
where $\mathbf{w_0}$ is the weight vector before regularization.

After some trials, we set the parameters and trained the model with all the 5000 samples.
\subsection{Regularized Neural Network with Momentum}
We used 5000 training data to train our old model, and used the 10000 \emph{test\underline{ }batch}  data in the CIFAR-10 batch data to test.
The change of parameters such as number of nodes, converge condition, and regularization weight could hardly make progress. We found it necessary to upgrade the complexity of our model. We took advantage of the methods used in deep learning, i.e. increased the number of hidden layers.

In general, the regularization term we used remained the same - Simple Weight Decay. Instead of using the learning rate function in the second model, we set a fixed learning rate at:
\begin{equation}
\eta = 0.01 
\end{equation}
This was a result of trial and error. And also we used a fixed number of epochs.
In addition, we added Momentum to stochastic gradient descent to diminish the fluctuations in weight changes over consecutive iterations:
\begin{equation}
\theta \leftarrow \theta - \eta \Delta
\end{equation}
\begin{equation}
\Delta \leftarrow 0.9 \Delta + \frac{\partial L}{\partial \theta}
\end{equation}

As to increasing the number of hidden layers, we first tried adding just one layer. There was a slight improvement in the accuracy. And the accuracy was stable. Therefore, we modified our model to take a new parameter which indicated the number of hidden layers. And we explored iteratively.

\subsubsection{Results: Basic Neural Network}
The highest accuracy on local machine was 54.1\%. We submitted a final model of 52.8\% on Autolab and the result was 48.4\%.

\subsubsection{Results: Regularized Neural Network}
The accuracy on training data before regularization was around 90\%, but decreased to around 78\% after regularization. The accuracy on test data had a little increase, and the overfit problem was solved. And the test result on Autolab was an accuracy of 50\%.

\subsubsection{Results: Regularized Neural Network with Momentum}
The new activation function and loss function we adopted here improved the performance by about 8\% compared to the models we used before. At that time, we just had one hidden layer. As is mentioned in above subsection, we iteratively explored the number of hidden layers. The accuracy kept increasing in the first place but became not so stable as it got larger than four. Even we suffered a performance degradation after the number of hidden layers became larger than six. To maintain a relatively stable classifier, we chose three as our number of hidden layers. For the number of nodes in these three layers, we decided to use  150, 150 and 50 respectively. 

On local machine, the test result was some 64\% correctness. The submission result on Autolab was 62.7\%.

\section{Conclusion}
Although we just implemented relatively simple models of Kernel SVM and Neural Network, they perform pretty well on this task. We think this is because they're both non-linear classifiers. However, this tends to lead to over-fitting, as we observed in Neural Network, and a little bit in SVM. After tuning they should perform better.

We still plan to try Logistic Regression or Bayesian Network.



\bibliographystyle{abbrv}
\bibliography{nips2013}  % sigproc.bib is the name of the Bibliography
\end{document}
